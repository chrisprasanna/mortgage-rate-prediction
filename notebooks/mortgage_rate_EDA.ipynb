{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f222d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9421d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec56990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fredapi import Fred\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import io\n",
    "import feedparser\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50aa7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(1971, 4, 2).date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe9060",
   "metadata": {},
   "source": [
    "# FRED API Setup\n",
    "Visit https://fredaccount.stlouisfed.org/apikeyl, request key, and save it as `fred_token.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b920e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API from txt file and create Fred object\n",
    "with open('../fred_token.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "fred = Fred(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d692e9",
   "metadata": {},
   "source": [
    "# Fetch FRED Economic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7df2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fred_data(series_id, start_date=\"1971-04-02\"):\n",
    "    try:\n",
    "        data = fred.get_series(series_id, observation_start=start_date)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {series_id}: {e}\")\n",
    "        return None\n",
    "    df = pd.DataFrame(data, columns=[series_id])\n",
    "    df.index.name = \"Date\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_ids = {\n",
    "    \"MORTGAGE30US\": \"30-Year Fixed Mortgage Rate\",\n",
    "    \"DGS10\": \"10-Year Treasury Yield\",\n",
    "    \"FEDFUNDS\": \"Federal Funds Rate\",\n",
    "    \"CPIAUCSL\": \"Consumer Price Index (CPI)\",\n",
    "    \"UNRATE\": \"Unemployment Rate\",\n",
    "    \"GDP\": \"Gross Domestic Product (GDP)\",\n",
    "    # \"FIXHAI\": \"Housing Affordability Index\",\n",
    "    \"HOUST\": \"New Residential Construction\",\n",
    "    \"PERMIT\": \"Building Permits\",\n",
    "    'DFF': \"Discount Rate\",\n",
    "    'M2SL': \"Money Supply M2\",\n",
    "    'M1SL': \"Money Supply M1\",\n",
    "    'M1V': \"Money Velocity M1\",\n",
    "    'M2V': \"Money Velocity M2\",\n",
    "    'M3SL': \"Money Supply M3\",\n",
    "    'M2REAL': \"Real M2 Money Stock\",\n",
    "    'M1REAL': \"Real M1 Money Stock\",\n",
    "    'M2REAL': \"Real M2 Money Stock\",\n",
    "    'M1REAL': \"Real M1 Money Stock\",\n",
    "    'PAYEMS': \"All Employees: Total Nonfarm Payrolls\",\n",
    "    'CIVPART': \"Civilian Labor Force Participation Rate\",\n",
    "    # 'JTSJOL': \"Job Openings: Total Nonfarm\",\n",
    "    \"PCEPI\": \"Personal Consumption Expenditures Price Index\",\n",
    "    \"CPILFESL\": \"CPI for All Urban Consumers: Food\",\n",
    "    \"PPIACO\": \"Producer Price Index for All Commodities\",\n",
    "    # \"SPCS20RSA\": \"S&P/Case-Shiller 20-City Composite Home Price Index\",\n",
    "    \"MSPUS\": \"Median Sales Price of Houses Sold for the United States\",\n",
    "    \"GDPC1\": \"Real Gross Domestic Product\",\n",
    "    \"GNPCA\": \"Real Gross National Product\",\n",
    "    \"A939RC0Q052SBEA\": \"Federal Government Current Expenditures\",\n",
    "    \"EXPGS\": \"Exports of Goods and Services\",\n",
    "    # \"IR14240\": \"Effective Federal Funds Rate\",\n",
    "    # \"BOPGSTB\": \"Balance of Payments: Goods and Services\",\n",
    "    \"TOTRESNS\": \"Total Reserves of Depository Institutions\",\n",
    "    \"BUSLOANS\": \"Commercial and Industrial Loans\",\n",
    "    # \"RETAILSMSA\": \"Retail and Food Services Sales\",\n",
    "    \"UMCSENT\": \"University of Michigan Consumer Sentiment\",\n",
    "    \"PCEPI\": \"Personal Consumption Expenditures Price Index\",\n",
    "    \"USEPUINDXD\": \"Economic Policy Uncertainty Index for the United States\",\n",
    "}\n",
    "data_frames = {name: fetch_fred_data(code) for code, name in series_ids.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d6294",
   "metadata": {},
   "source": [
    "# Stock & Bond Market Indicators (via Yahoo Finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_tickers = [\n",
    "    '^GSPC', # S&P 500\n",
    "    '^VIX', # VIX\n",
    "    # 'TLT', # 20+ Year Treasury ETF\n",
    "    # 'MBS', # Mortgage-Backed Securities\n",
    "    'FNMA', # Fannie Mae 30-Year Fixed Rate Mortgage\n",
    "    ]\n",
    "\n",
    "yahoo_data = yf.download(yahoo_tickers, start=\"1971-04-02\", group_by='ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ac97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb244b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten MultiIndex if needed\n",
    "if isinstance(yahoo_data.columns, pd.MultiIndex):\n",
    "    flat_data = yahoo_data.stack(level=1).rename_axis(['Date', 'Ticker']).reset_index()\n",
    "    flat_data = flat_data.pivot(index='Date', columns='Ticker')\n",
    "    flat_data.columns = ['{}_{}'.format(ticker, col) for col, ticker in flat_data.columns]\n",
    "    flat_data.index = pd.to_datetime(flat_data.index)\n",
    "else:\n",
    "    flat_data = yahoo_data.copy()\n",
    "\n",
    "# Only keep Close prices\n",
    "flat_data = flat_data.filter(like='Close')\n",
    "# Rename columns for clarity\n",
    "flat_data.columns = [col.replace('Close_', '') for col in flat_data.columns]\n",
    "\n",
    "# Add to data_frames for merging\n",
    "data_frames.update(flat_data.to_dict(orient='series'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853f567",
   "metadata": {},
   "source": [
    "# Zillow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_url = \"https://files.zillowstatic.com/research/public_csvs/zhvi/Metro_zhvi_uc_sfr_tier_0.33_0.67_sm_sa_month.csv?t=1744322036\"\n",
    "zillow_data = pd.read_csv(zillow_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155de45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zillow_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e13487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Seattle, WA and transpose the data to have dates as the index\n",
    "zillow_data = zillow_data[zillow_data['RegionName'] == 'Seattle, WA']\n",
    "zillow_data = zillow_data.drop(columns=['RegionID', 'SizeRank', 'RegionType', 'StateName']).set_index('RegionName').T\n",
    "zillow_data.index = pd.to_datetime(zillow_data.index)\n",
    "zillow_data = zillow_data.rename(columns={'Seattle, WA': 'Zillow_HPI'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b368b",
   "metadata": {},
   "source": [
    "# News Sentiment\n",
    "https://tradewithpython.com/news-sentiment-analysis-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "config.request_timeout = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104df241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U.S. and finance-specific sources\n",
    "finance_domains = [\n",
    "    \"reuters.com\", \"bloomberg.com\", \"cnbc.com\", \"yahoo.com\",\n",
    "    \"foxbusiness.com\", \"marketwatch.com\", \"wsj.com\",\n",
    "    \"forbes.com\", \"businessinsider.com\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_date_range(start_date, end_date, delta_days=30):\n",
    "    \"\"\"Yield start and end date pairs split by delta_days chunks\"\"\"\n",
    "    current = start_date\n",
    "    while current < end_date:\n",
    "        chunk_end = min(current + timedelta(days=delta_days), end_date)\n",
    "        yield current, chunk_end\n",
    "        current = chunk_end + timedelta(days=1)\n",
    "\n",
    "def get_nltk_news_sentiment(topic=\"mortgage rates\", start_date=datetime(2024, 1, 1)):\n",
    "    today = datetime.today().date()\n",
    "    all_results = []\n",
    "\n",
    "    for domain in finance_domains:\n",
    "        for chunk_start, chunk_end in chunk_date_range(start_date, today):\n",
    "            start_str = chunk_start.strftime('%m/%d/%Y')\n",
    "            end_str = chunk_end.strftime('%m/%d/%Y')\n",
    "            search_query = f\"{topic} site:{domain}\"\n",
    "\n",
    "            print(f\"\\nðŸ” Searching {search_query} from {start_str} to {end_str}\")\n",
    "\n",
    "            googlenews = GoogleNews(lang='en', start=start_str, end=end_str)\n",
    "            googlenews.search(search_query)\n",
    "\n",
    "            for i in range(1, 6):  # Get first 5 pages per query\n",
    "                try:\n",
    "                    googlenews.getpage(i)\n",
    "                    page_results = googlenews.results()\n",
    "                    if not page_results:\n",
    "                        break\n",
    "                    all_results.extend(page_results)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching page {i} for {domain}: {e}\")\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"âŒ No results from any financial sources.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_results).drop_duplicates(subset=\"link\")\n",
    "    print(f\"\\nâœ… Collected {len(df)} unique articles\")\n",
    "\n",
    "    records = []\n",
    "    for i, row in df.iterrows():\n",
    "        url = row.get('link')\n",
    "        if pd.isna(url):\n",
    "            continue\n",
    "        if \"&ved=\" in url:\n",
    "            url = url.split(\"&ved=\")[0]\n",
    "\n",
    "        try:\n",
    "            article = Article(url, config=config)\n",
    "            article.download()\n",
    "            time.sleep(1)\n",
    "            article.parse()\n",
    "\n",
    "            text = article.text\n",
    "            summary = text[:500] if text else \"\"\n",
    "            title = article.title\n",
    "\n",
    "            if not text and not title:\n",
    "                continue\n",
    "\n",
    "            article_date = row.get('datetime') or datetime.now()\n",
    "\n",
    "            records.append({\n",
    "                'Date': article_date,\n",
    "                'Media': row.get('media', 'Unknown'),\n",
    "                'Title': title,\n",
    "                'Summary': summary\n",
    "            })\n",
    "            print(f\"âœ… Parsed: {title[:60]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipping article due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not records:\n",
    "        print(\"âŒ No articles successfully processed\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    news_df = pd.DataFrame(records)\n",
    "\n",
    "    # Ensure Date column is datetime\n",
    "    news_df['Date'] = pd.to_datetime(news_df['Date'], errors='coerce')\n",
    "    news_df = news_df.dropna(subset=['Date'])\n",
    "    news_df['Week'] = news_df['Date'].dt.to_period('W').dt.start_time\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    news_df['Summary'] = news_df['Summary'].fillna('')\n",
    "    news_df['Sentiment'] = news_df['Summary'].apply(lambda x: sia.polarity_scores(x))\n",
    "    news_df['Compound'] = news_df['Sentiment'].apply(lambda x: x['compound'])\n",
    "    news_df['Pos'] = news_df['Sentiment'].apply(lambda x: x['pos'])\n",
    "    news_df['Neg'] = news_df['Sentiment'].apply(lambda x: x['neg'])\n",
    "    news_df['Neu'] = news_df['Sentiment'].apply(lambda x: x['neu'])\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ Performing weekly aggregation...\")\n",
    "    try:\n",
    "        weekly = news_df.groupby(\"Week\").agg(\n",
    "            NewsSentiment=(\"Compound\", \"mean\"),\n",
    "            NewsPos=(\"Pos\", \"mean\"),\n",
    "            NewsNeg=(\"Neg\", \"mean\"),\n",
    "            NewsNeu=(\"Neu\", \"mean\"),\n",
    "            NewsCount=(\"Compound\", \"count\")\n",
    "        )\n",
    "        print(f\"âœ… Aggregated {len(weekly)} weeks of sentiment data\")\n",
    "        return weekly\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Aggregation failed: {e}\")\n",
    "        return news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc59d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_df = get_nltk_news_sentiment(\"mortgage rates\", start_date=start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794aaca",
   "metadata": {},
   "source": [
    "# Merge All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([df for df in data_frames.values()], axis=1)\n",
    "\n",
    "# Zillow data only available in 21st century. Skip it for now.\n",
    "# combined = pd.concat([combined, zillow_data], axis=1)\n",
    "\n",
    "combined = combined.resample(\"W\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed730e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Sentiment Features (per week)\n",
    "# combined = combined.merge(sentiment_df, how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e98bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['MORTGAGE30US_diff'] = combined['MORTGAGE30US'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e95e0",
   "metadata": {},
   "source": [
    "# Forward and Back Fill\n",
    "May want to comment out if planning to use tree-based models since they can handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with forward fill method\n",
    "combined.ffill(inplace=True)\n",
    "\n",
    "# # Fill any remaining NaNs with backward fill method\n",
    "# combined.bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142deb2",
   "metadata": {},
   "source": [
    "# EDA Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ed6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid layout for subplots\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(8, 22))\n",
    "\n",
    "# Plot variables with similar scales together\n",
    "axes[0].set_title(\"Mortgage Rate and Treasury Yield\")\n",
    "axes[0].plot(combined.index, combined[\"MORTGAGE30US\"], label=\"Mortgage Rate\")\n",
    "axes[0].plot(combined.index, combined[\"DGS10\"], label=\"10-Year Treasury Yield\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].set_title(\"Federal Funds Rate and Unemployment Rate\")\n",
    "axes[1].plot(combined.index, combined[\"FEDFUNDS\"], label=\"Federal Funds Rate\", color=\"green\")\n",
    "axes[1].plot(combined.index, combined[\"UNRATE\"], label=\"Unemployment Rate\", color=\"orange\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "axes[2].set_title(\"Consumer Price Index (CPI)\")\n",
    "axes[2].plot(combined.index, combined[\"CPIAUCSL\"], label=\"CPI\", color=\"purple\")\n",
    "axes[2].legend()\n",
    "axes[2].grid()\n",
    "\n",
    "axes[3].set_title(\"GDP\")\n",
    "axes[3].plot(combined.index, combined[\"GDP\"], label=\"GDP\", color=\"blue\")\n",
    "axes[3].legend()\n",
    "axes[3].grid()\n",
    "\n",
    "axes[5].set_title(\"S&P 500\")\n",
    "axes[5].plot(combined.index, combined[\"^GSPC\"], label=\"S&P 500\", color=\"brown\")\n",
    "axes[5].legend()\n",
    "axes[5].grid()\n",
    "\n",
    "axes[6].set_title(\"Market Indicators (VIX)\")\n",
    "axes[6].plot(combined.index, combined[\"^VIX\"], label=\"VIX\", color=\"cyan\")\n",
    "axes[6].legend()\n",
    "axes[6].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23137ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(combined.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "# plt.title(\"Feature Correlation Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43c48c",
   "metadata": {},
   "source": [
    "# Lag Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355141fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [1, 2, 4, 8]:\n",
    "    combined[f\"MORTGAGE30US_{lag}\"] = combined[\"MORTGAGE30US\"].shift(lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba573d",
   "metadata": {},
   "source": [
    "# Save Data Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ee24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"../data/full_mortgage_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
